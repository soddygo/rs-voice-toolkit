//! Voice Toolkit æµå¼è½¬å½•ç¤ºä¾‹
//! 
//! è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ voice-toolkit è¿›è¡Œå®æ—¶éŸ³é¢‘æµè½¬å½•ã€‚
//! 
//! # ä½¿ç”¨è¯´æ˜
//! 
//! è¿è¡Œè¿™ä¸ªç¤ºä¾‹éœ€è¦ï¼š
//! 1. å¯ç”¨ streaming ç‰¹æ€§: `cargo run --example streaming_example --features streaming`
//! 2. æä¾› Whisper æ¨¡å‹è·¯å¾„å’ŒéŸ³é¢‘æ–‡ä»¶è·¯å¾„

use voice_toolkit::Result;

/// æµå¼è½¬å½•ç¤ºä¾‹
/// 
/// è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ StreamingTranscriber è¿›è¡Œå®æ—¶éŸ³é¢‘æµè½¬å½•ã€‚
/// 
/// # å‚æ•°
/// 
/// * `model_path` - Whisper æ¨¡å‹æ–‡ä»¶çš„è·¯å¾„
/// * `audio_path` - è¦è½¬å½•çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
/// 
/// # ç¤ºä¾‹
/// 
/// ```rust
/// # use voice_toolkit::streaming_examples::streaming_transcription_example;
/// # tokio_test::block_on(async {
/// streaming_transcription_example("models/ggml-base.bin", "audio/hello.wav").await.unwrap();
/// # });
/// ```
#[cfg(feature = "streaming")]
pub async fn streaming_transcription_example<P1, P2>(model_path: P1, audio_path: P2) -> Result<()>
where
    P1: AsRef<Path>,
    P2: AsRef<Path>,
{
    use voice_toolkit::stt::streaming::StreamingTranscriber;
    use voice_toolkit::audio::get_audio_metadata;
    
    println!("ğŸ™ï¸  å¼€å§‹æµå¼è½¬å½•ç¤ºä¾‹");
    println!("æ¨¡å‹: {:?}", model_path.as_ref());
    println!("éŸ³é¢‘: {:?}", audio_path.as_ref());
    
    // è·å–éŸ³é¢‘æ–‡ä»¶ä¿¡æ¯
    let metadata = get_audio_metadata(audio_path.as_ref()).await?;
    println!("éŸ³é¢‘ä¿¡æ¯: {:.2}s, {}Hz, {} å£°é“", 
        metadata.duration, metadata.sample_rate, metadata.channels);
    
    // åˆ›å»ºæµå¼è½¬å½•å™¨
    let mut transcriber = StreamingTranscriber::new(model_path).await?;
    
    // é…ç½®è½¬å½•å‚æ•°
    transcriber.set_language("auto")?; // è‡ªåŠ¨æ£€æµ‹è¯­è¨€
    transcriber.set_task("transcribe")?; // è½¬å½•ä»»åŠ¡
    
    println!("ğŸ§ å¼€å§‹å¤„ç†éŸ³é¢‘æµ...");
    
    // è¯»å–éŸ³é¢‘æ–‡ä»¶å¹¶æ¨¡æ‹Ÿæµå¼å¤„ç†
    let audio_data = std::fs::read(audio_path.as_ref())?;
    let chunk_size = 1024 * 4; // 4KB chunks
    
    let mut position = 0;
    let mut total_segments = 0;
    let mut start_time = std::time::Instant::now();
    
    while position < audio_data.len() {
        let end_position = (position + chunk_size).min(audio_data.len());
        let chunk = &audio_data[position..end_position];
        
        // å¤„ç†éŸ³é¢‘å—
        let segments = transcriber.process_audio(chunk).await?;
        
        // æ˜¾ç¤ºæ–°çš„è½¬å½•ç»“æœ
        for segment in segments {
            total_segments += 1;
            println!("ğŸ“ [{}s - {}s] {}", 
                segment.start_time, 
                segment.end_time, 
                segment.text);
        }
        
        position = end_position;
        
        // æ¨¡æ‹Ÿå®æ—¶å¤„ç†å»¶è¿Ÿ
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
    }
    
    // è·å–æœ€ç»ˆç»“æœ
    let final_result = transcriber.finalize().await?;
    
    let processing_time = start_time.elapsed();
    
    println!("\nâœ… æµå¼è½¬å½•å®Œæˆ!");
    println!("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:");
    println!("  æ€»æ®µè½æ•°: {}", total_segments);
    println!("  æœ€ç»ˆæ–‡æœ¬: {}", final_result.text);
    println!("  å¤„ç†æ—¶é—´: {:?}", processing_time);
    println!("  å®æ—¶å› å­: {:.3}", 
        processing_time.as_secs_f64() / metadata.duration);
    
    Ok(())
}

/// å¸¦VADçš„æµå¼è½¬å½•ç¤ºä¾‹
/// 
/// è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ç»“åˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹(VAD)è¿›è¡Œæ›´é«˜æ•ˆçš„æµå¼è½¬å½•ã€‚
/// 
/// # å‚æ•°
/// 
/// * `model_path` - Whisper æ¨¡å‹æ–‡ä»¶çš„è·¯å¾„
/// * `audio_path` - è¦è½¬å½•çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
/// 
/// # ç¤ºä¾‹
/// 
/// ```rust
/// # use voice_toolkit::streaming_examples::streaming_with_vad_example;
/// # tokio_test::block_on(async {
/// streaming_with_vad_example("models/ggml-base.bin", "audio/hello.wav").await.unwrap();
/// # });
/// ```
#[cfg(feature = "streaming")]
pub async fn streaming_with_vad_example<P1, P2>(model_path: P1, audio_path: P2) -> Result<()>
where
    P1: AsRef<Path>,
    P2: AsRef<Path>,
{
    use voice_toolkit::stt::streaming::StreamingTranscriber;
    use voice_toolkit::audio::get_audio_metadata;
    
    println!("ğŸ™ï¸  å¼€å§‹å¸¦VADçš„æµå¼è½¬å½•ç¤ºä¾‹");
    println!("æ¨¡å‹: {:?}", model_path.as_ref());
    println!("éŸ³é¢‘: {:?}", audio_path.as_ref());
    
    // è·å–éŸ³é¢‘æ–‡ä»¶ä¿¡æ¯
    let metadata = get_audio_metadata(audio_path.as_ref()).await?;
    println!("éŸ³é¢‘ä¿¡æ¯: {:.2}s, {}Hz, {} å£°é“", 
        metadata.duration, metadata.sample_rate, metadata.channels);
    
    // åˆ›å»ºæµå¼è½¬å½•å™¨å¹¶å¯ç”¨VAD
    let mut transcriber = StreamingTranscriber::new(model_path).await?;
    
    // å¯ç”¨VAD
    transcriber.enable_vad(true)?;
    transcriber.set_vad_threshold(0.5)?; // è®¾ç½®VADé˜ˆå€¼
    
    println!("ğŸ§ å¼€å§‹å¤„ç†éŸ³é¢‘æµ (VADå·²å¯ç”¨)...");
    
    // è¯»å–éŸ³é¢‘æ–‡ä»¶å¹¶æ¨¡æ‹Ÿæµå¼å¤„ç†
    let audio_data = std::fs::read(audio_path.as_ref())?;
    let chunk_size = 1024 * 2; // è¾ƒå°çš„å—å¤§å°ï¼Œæ›´é€‚åˆVAD
    
    let mut position = 0;
    let mut voice_segments = 0;
    let mut silence_segments = 0;
    let mut total_segments = 0;
    let mut start_time = std::time::Instant::now();
    
    while position < audio_data.len() {
        let end_position = (position + chunk_size).min(audio_data.len());
        let chunk = &audio_data[position..end_position];
        
        // å¤„ç†éŸ³é¢‘å—
        let segments = transcriber.process_audio(chunk).await?;
        
        // æ£€æŸ¥VADçŠ¶æ€
        let is_speech = transcriber.is_speech_detected().unwrap_or(false);
        
        if is_speech {
            voice_segments += 1;
            println!("ğŸ—£ï¸  æ£€æµ‹åˆ°è¯­éŸ³ - å¤„ç† {} ä¸ªæ®µè½", segments.len());
        } else {
            silence_segments += 1;
            if !segments.is_empty() {
                println!("ğŸ¤« é™éŸ³æ®µ - ä»æœ‰ {} ä¸ªå¾…å¤„ç†æ®µè½", segments.len());
            }
        }
        
        // æ˜¾ç¤ºè½¬å½•ç»“æœ
        for segment in segments {
            total_segments += 1;
            println!("ğŸ“ [{}s - {}s] {}", 
                segment.start_time, 
                segment.end_time, 
                segment.text);
        }
        
        position = end_position;
        
        // æ¨¡æ‹Ÿå®æ—¶å¤„ç†å»¶è¿Ÿ
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
    }
    
    // è·å–æœ€ç»ˆç»“æœ
    let final_result = transcriber.finalize().await?;
    
    let processing_time = start_time.elapsed();
    
    println!("\nâœ… å¸¦VADçš„æµå¼è½¬å½•å®Œæˆ!");
    println!("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:");
    println!("  è¯­éŸ³æ®µè½æ•°: {}", voice_segments);
    println!("  é™éŸ³æ®µè½æ•°: {}", silence_segments);
    println!("  æ€»è½¬å½•æ®µè½æ•°: {}", total_segments);
    println!("  æœ€ç»ˆæ–‡æœ¬: {}", final_result.text);
    println!("  å¤„ç†æ—¶é—´: {:?}", processing_time);
    println!("  å®æ—¶å› å­: {:.3}", 
        processing_time.as_secs_f64() / metadata.duration);
    
    // VADæ•ˆç‡åˆ†æ
    let total_chunks = voice_segments + silence_segments;
    let voice_ratio = voice_segments as f64 / total_chunks as f64;
    println!("  è¯­éŸ³å æ¯”: {:.1}%", voice_ratio * 100.0);
    
    if voice_ratio < 0.3 {
        println!("  ğŸ’¡ VADä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼Œè·³è¿‡äº† {:.1}% çš„é™éŸ³éƒ¨åˆ†", 
            (1.0 - voice_ratio) * 100.0);
    }
    
    Ok(())
}

/// å®æ—¶éº¦å…‹é£è½¬å½•ç¤ºä¾‹
/// 
/// è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä»éº¦å…‹é£è¿›è¡Œå®æ—¶è½¬å½•ã€‚
/// 
/// # æ³¨æ„äº‹é¡¹
/// 
/// è¿™ä¸ªç¤ºä¾‹éœ€è¦é¢å¤–çš„éŸ³é¢‘è¾“å…¥åº“ï¼Œè¿™é‡Œåªæä¾›æ¡†æ¶ä»£ç ã€‚
/// 
/// # ç¤ºä¾‹
/// 
/// ```rust
/// # use voice_toolkit::streaming_examples::real_time_microphone_example;
/// # tokio_test::block_on(async {
/// // æ³¨æ„: è¿™ä¸ªç¤ºä¾‹éœ€è¦å®é™…çš„éŸ³é¢‘è¾“å…¥åº“æ”¯æŒ
/// // real_time_microphone_example("models/ggml-base.bin").await.unwrap();
/// # });
/// ```
#[cfg(feature = "streaming")]
pub async fn real_time_microphone_example<P: AsRef<Path>>(model_path: P) -> Result<()> {
    use voice_toolkit::stt::streaming::StreamingTranscriber;
    
    println!("ğŸ™ï¸  å¼€å§‹å®æ—¶éº¦å…‹é£è½¬å½•ç¤ºä¾‹");
    println!("æ¨¡å‹: {:?}", model_path.as_ref());
    println!("âš ï¸  æ³¨æ„: è¿™ä¸ªç¤ºä¾‹éœ€è¦éŸ³é¢‘è¾“å…¥åº“æ”¯æŒ");
    
    // åˆ›å»ºæµå¼è½¬å½•å™¨
    let mut transcriber = StreamingTranscriber::new(model_path).await?;
    
    // é…ç½®å‚æ•°
    transcriber.set_language("auto")?;
    transcriber.set_task("transcribe")?;
    transcriber.enable_vad(true)?;
    
    println!("ğŸ§ å¼€å§‹ç›‘å¬éº¦å…‹é£...");
    println!("æŒ‰ Ctrl+C åœæ­¢è½¬å½•");
    
    // æ¨¡æ‹ŸéŸ³é¢‘è¾“å…¥å¾ªç¯
    // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥ä»éº¦å…‹é£è¯»å–éŸ³é¢‘æ•°æ®
    let mut counter = 0;
    let start_time = std::time::Instant::now();
    
    loop {
        counter += 1;
        
        // æ¨¡æ‹ŸéŸ³é¢‘æ•°æ®ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ä»éº¦å…‹é£è¯»å–ï¼‰
        let fake_audio_data = vec![0u8; 1024];
        
        // å¤„ç†éŸ³é¢‘æ•°æ®
        match transcriber.process_audio(&fake_audio_data).await {
            Ok(segments) => {
                for segment in segments {
                    println!("ğŸ“ [{}s - {}s] {}", 
                        segment.start_time, 
                        segment.end_time, 
                        segment.text);
                }
            }
            Err(e) => {
                eprintln!("å¤„ç†éŸ³é¢‘æ•°æ®æ—¶å‡ºé”™: {}", e);
            }
        }
        
        // æ¨¡æ‹Ÿå®æ—¶å¤„ç†
        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
        
        // æ¨¡æ‹Ÿåœæ­¢æ¡ä»¶
        if counter >= 50 { // 5ç§’ååœæ­¢
            break;
        }
    }
    
    // è·å–æœ€ç»ˆç»“æœ
    let final_result = transcriber.finalize().await?;
    
    println!("\nâœ… å®æ—¶è½¬å½•å®Œæˆ!");
    println!("è¿è¡Œæ—¶é—´: {:?}", start_time.elapsed());
    println!("æœ€ç»ˆæ–‡æœ¬: {}", final_result.text);
    
    Ok(())
}

/// æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹
/// 
/// è¿™ä¸ªç¤ºä¾‹å¯¹æ¯”äº†æµå¼è½¬å½•å’Œæ‰¹é‡è½¬å½•çš„æ€§èƒ½å·®å¼‚ã€‚
/// 
/// # å‚æ•°
/// 
/// * `model_path` - Whisper æ¨¡å‹æ–‡ä»¶çš„è·¯å¾„
/// * `audio_path` - è¦è½¬å½•çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
/// 
/// # ç¤ºä¾‹
/// 
/// ```rust
/// # use voice_toolkit::streaming_examples::performance_comparison_example;
/// # tokio_test::block_on(async {
/// performance_comparison_example("models/ggml-base.bin", "audio/hello.wav").await.unwrap();
/// # });
/// ```
#[cfg(feature = "streaming")]
pub async fn performance_comparison_example<P1, P2>(model_path: P1, audio_path: P2) -> Result<()>
where
    P1: AsRef<Path>,
    P2: AsRef<Path>,
{
    use voice_toolkit::stt::streaming::StreamingTranscriber;
    use voice_toolkit::audio::get_audio_metadata;
    
    println!("ğŸ” å¼€å§‹æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹");
    println!("æ¨¡å‹: {:?}", model_path.as_ref());
    println!("éŸ³é¢‘: {:?}", audio_path.as_ref());
    
    // è·å–éŸ³é¢‘æ–‡ä»¶ä¿¡æ¯
    let metadata = get_audio_metadata(audio_path.as_ref()).await?;
    println!("éŸ³é¢‘ä¿¡æ¯: {:.2}s, {}Hz, {} å£°é“", 
        metadata.duration, metadata.sample_rate, metadata.channels);
    
    // æµ‹è¯•æ‰¹é‡è½¬å½•
    println!("\nğŸ“Š æ‰¹é‡è½¬å½•æµ‹è¯•:");
    let batch_start = std::time::Instant::now();
    let batch_result = transcribe_file_unified(model_path.as_ref(), audio_path.as_ref()).await?;
    let batch_time = batch_start.elapsed();
    
    println!("  æ‰¹é‡è½¬å½•æ—¶é—´: {:?}", batch_time);
    println!("  æ‰¹é‡è½¬å½•ç»“æœ: {}", batch_result.text);
    println!("  æ‰¹é‡è½¬å½•RTF: {:.3}", 
        batch_time.as_secs_f64() / metadata.duration);
    
    // æµ‹è¯•æµå¼è½¬å½•
    println!("\nğŸ“Š æµå¼è½¬å½•æµ‹è¯•:");
    let mut transcriber = StreamingTranscriber::new(model_path.as_ref()).await?;
    
    let stream_start = std::time::Instant::now();
    
    // è¯»å–éŸ³é¢‘æ–‡ä»¶
    let audio_data = std::fs::read(audio_path.as_ref())?;
    let chunk_size = 1024 * 4;
    
    let mut position = 0;
    let mut stream_segments = Vec::new();
    
    while position < audio_data.len() {
        let end_position = (position + chunk_size).min(audio_data.len());
        let chunk = &audio_data[position..end_position];
        
        let segments = transcriber.process_audio(chunk).await?;
        stream_segments.extend(segments);
        
        position = end_position;
        
        // æ¨¡æ‹Ÿå®æ—¶å¤„ç†å»¶è¿Ÿ
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
    }
    
    let stream_result = transcriber.finalize().await?;
    let stream_time = stream_start.elapsed();
    
    println!("  æµå¼è½¬å½•æ—¶é—´: {:?}", stream_time);
    println!("  æµå¼è½¬å½•ç»“æœ: {}", stream_result.text);
    println!("  æµå¼è½¬å½•RTF: {:.3}", 
        stream_time.as_secs_f64() / metadata.duration);
    println!("  æµå¼æ®µè½æ•°: {}", stream_segments.len());
    
    // æ€§èƒ½å¯¹æ¯”åˆ†æ
    println!("\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”åˆ†æ:");
    println!("  æ‰¹é‡è½¬å½•æ—¶é—´: {:.2}s", batch_time.as_secs_f64());
    println!("  æµå¼è½¬å½•æ—¶é—´: {:.2}s", stream_time.as_secs_f64());
    println!("  æ—¶é—´å·®å¼‚: {:.2}s", 
        (stream_time.as_secs_f64() - batch_time.as_secs_f64()).abs());
    
    let time_ratio = stream_time.as_secs_f64() / batch_time.as_secs_f64();
    println!("  æ—¶é—´æ¯”ç‡: {:.3}", time_ratio);
    
    if time_ratio < 1.1 {
        println!("  âœ… æµå¼è½¬å½•æ€§èƒ½æ¥è¿‘æ‰¹é‡è½¬å½•");
    } else if time_ratio < 1.5 {
        println!("  âš ï¸  æµå¼è½¬å½•æœ‰è½»å¾®æ€§èƒ½æŸå¤±");
    } else {
        println!("  âŒ æµå¼è½¬å½•æ€§èƒ½æŸå¤±è¾ƒå¤§");
    }
    
    // å‡†ç¡®æ€§å¯¹æ¯”
    let batch_text = batch_result.text.trim();
    let stream_text = stream_result.text.trim();
    
    println!("\nğŸ¯ å‡†ç¡®æ€§å¯¹æ¯”:");
    println!("  æ‰¹é‡è½¬å½•: {}", batch_text);
    println!("  æµå¼è½¬å½•: {}", stream_text);
    
    if batch_text == stream_text {
        println!("  âœ… è½¬å½•ç»“æœå®Œå…¨ä¸€è‡´");
    } else {
        println!("  âš ï¸  è½¬å½•ç»“æœå­˜åœ¨å·®å¼‚");
        
        // è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„å­—ç¬¦åŒ¹é…ï¼‰
        let similarity = calculate_similarity(batch_text, stream_text);
        println!("  ç›¸ä¼¼åº¦: {:.1}%", similarity * 100.0);
    }
    
    Ok(())
}

/// è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦
#[allow(dead_code)]
fn calculate_similarity(s1: &str, s2: &str) -> f64 {
    if s1.is_empty() || s2.is_empty() {
        return 0.0;
    }
    
    // ä½¿ç”¨ç¼–è¾‘è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦
    let distance = levenshtein_distance(s1, s2);
    let max_len = s1.len().max(s2.len());
    
    1.0 - (distance as f64 / max_len as f64)
}

/// è®¡ç®—ç¼–è¾‘è·ç¦»
#[allow(dead_code)]
fn levenshtein_distance(s1: &str, s2: &str) -> usize {
    let s1: Vec<char> = s1.chars().collect();
    let s2: Vec<char> = s2.chars().collect();
    
    let mut matrix = vec![vec![0; s2.len() + 1]; s1.len() + 1];
    
    for i in 0..=s1.len() {
        matrix[i][0] = i;
    }
    
    for j in 0..=s2.len() {
        matrix[0][j] = j;
    }
    
    for i in 1..=s1.len() {
        for j in 1..=s2.len() {
            let cost = if s1[i - 1] == s2[j - 1] { 0 } else { 1 };
            matrix[i][j] = (matrix[i - 1][j] + 1)
                .min(matrix[i][j - 1] + 1)
                .min(matrix[i - 1][j - 1] + cost);
        }
    }
    
    matrix[s1.len()][s2.len()]
}

/// ä¸»å‡½æ•° - æ¼”ç¤ºæµå¼è½¬å½•ç¤ºä¾‹çš„ä½¿ç”¨
#[tokio::main]
async fn main() -> Result<()> {
    println!("ğŸ™ï¸  Voice Toolkit æµå¼è½¬å½•ç¤ºä¾‹");
    println!("{}", "=".repeat(50));
    
    // æ¨¡æ‹Ÿæ–‡ä»¶è·¯å¾„
    let _model_path = "models/ggml-base.bin";
    let _audio_path = "audio/hello.wav";
    
    // ç¤ºä¾‹ 1: åŸºæœ¬æµå¼è½¬å½•
    println!("\nğŸ“‹ ç¤ºä¾‹ 1: åŸºæœ¬æµå¼è½¬å½•");
    println!("è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†åŸºæœ¬çš„æµå¼è½¬å½•åŠŸèƒ½");
    println!("æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºï¼Œå®é™…ä½¿ç”¨éœ€è¦å¯ç”¨ streaming ç‰¹æ€§");
    
    #[cfg(feature = "streaming")]
    {
        match streaming_transcription_example(_model_path, _audio_path).await {
            Ok(_) => println!("æµå¼è½¬å½•ç¤ºä¾‹å®Œæˆ"),
            Err(e) => println!("æµå¼è½¬å½•ç¤ºä¾‹å¤±è´¥: {}", e),
        }
    }
    
    #[cfg(not(feature = "streaming"))]
    {
        println!("âš ï¸  streaming ç‰¹æ€§æœªå¯ç”¨ï¼Œæ— æ³•è¿è¡Œæµå¼è½¬å½•ç¤ºä¾‹");
        println!("è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œ:");
        println!("cargo run --example streaming_examples --features streaming");
    }
    
    // ç¤ºä¾‹ 2: å¸¦VADçš„æµå¼è½¬å½•
    println!("\nğŸ“‹ ç¤ºä¾‹ 2: å¸¦VADçš„æµå¼è½¬å½•");
    println!("è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ç»“åˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹è¿›è¡Œæµå¼è½¬å½•");
    
    #[cfg(feature = "streaming")]
    {
        match streaming_with_vad_example(model_path, audio_path).await {
            Ok(_) => println!("å¸¦VADçš„æµå¼è½¬å½•ç¤ºä¾‹å®Œæˆ"),
            Err(e) => println!("å¸¦VADçš„æµå¼è½¬å½•ç¤ºä¾‹å¤±è´¥: {}", e),
        }
    }
    
    // ç¤ºä¾‹ 3: æ€§èƒ½å¯¹æ¯”
    println!("\nğŸ“‹ ç¤ºä¾‹ 3: æ€§èƒ½å¯¹æ¯”");
    println!("è¿™ä¸ªç¤ºä¾‹å¯¹æ¯”äº†æµå¼è½¬å½•å’Œæ‰¹é‡è½¬å½•çš„æ€§èƒ½");
    
    #[cfg(feature = "streaming")]
    {
        match performance_comparison_example(model_path, audio_path).await {
            Ok(_) => println!("æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹å®Œæˆ"),
            Err(e) => println!("æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹å¤±è´¥: {}", e),
        }
    }
    
    // ç¤ºä¾‹ 4: å®æ—¶éº¦å…‹é£è½¬å½•
    println!("\nğŸ“‹ ç¤ºä¾‹ 4: å®æ—¶éº¦å…‹é£è½¬å½•");
    println!("è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä»éº¦å…‹é£è¿›è¡Œå®æ—¶è½¬å½•");
    println!("æ³¨æ„: è¿™ä¸ªç¤ºä¾‹éœ€è¦é¢å¤–çš„éŸ³é¢‘è¾“å…¥åº“æ”¯æŒ");
    
    #[cfg(feature = "streaming")]
    {
        println!("(è¿™æ˜¯ä¸€ä¸ªæ¡†æ¶ç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨éœ€è¦éŸ³é¢‘è¾“å…¥åº“)");
    }
    
    println!("\nğŸ‰ æ‰€æœ‰æµå¼è½¬å½•ç¤ºä¾‹æ¼”ç¤ºå®Œæˆ!");
    println!("æŸ¥çœ‹ä»£ç äº†è§£å¦‚ä½•åœ¨å®é™…åº”ç”¨ä¸­ä½¿ç”¨è¿™äº›åŠŸèƒ½");
    
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_levenshtein_distance() {
        assert_eq!(levenshtein_distance("kitten", "sitting"), 3);
        assert_eq!(levenshtein_distance("hello", "hello"), 0);
        assert_eq!(levenshtein_distance("", "abc"), 3);
        assert_eq!(levenshtein_distance("abc", ""), 3);
    }
    
    #[test]
    fn test_similarity() {
        assert!(calculate_similarity("hello", "hello") > 0.99);
        assert!(calculate_similarity("hello", "hell") > 0.8);
        assert!(calculate_similarity("hello", "world") < 0.5);
    }
}